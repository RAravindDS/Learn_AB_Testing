{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to calculate the **sample size** that we need for the test to have a **statistical significance** and also we'll see **how long do we need to run the test for?** based on the metrics. \n",
    "\n",
    "Again this is our usecase hypothesis: \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/our_hypothesis.png\" width=\"500px\"></p>\n",
    "\n",
    "\n",
    "After finding the sample size, we need to define \n",
    "1. When we will start the A/B test? \n",
    "2. How long will it run for? \n",
    "3. How many users will be exposesd? \n",
    "\n",
    "Thare are many reasons to finding those things, like your company already running a lot of **a/b test** or your new sample is already exposed to some other feature. So decide based on what you have. \n",
    "\n",
    "### Deciding the group sizes and duration\n",
    "\n",
    "This is the very crucial part because based on selecting the group size, you will roll out the new feature to all others. So the selected group size will replicate the whold group. So To decide the **test duration** and **group size**, we need to know: \n",
    "\n",
    "    * Minimum detectable effect (MDE)\n",
    "    * Power -  beta - type 2 error rate \n",
    "    * Significance level - p-value - type 1 error rate \n",
    "\n",
    "Before looking at this, please understand what is **test group** and **control group**\n",
    "\n",
    "**Test Group**: This group is exposed to the new or modified version of the thing being tested. This could be a new design for a website page, a different headline for an ad, or a variation in the wording of a call to action button.\n",
    "\n",
    "**Control Group**: This group serves as a baseline for comparison. They are exposed to the original version of the thing being tested. The control group helps to isolate the effect of the change being tested in the test group. By comparing the results of the two groups, you can see if the new version had a positive, negative, or neutral impact.\n",
    "\n",
    "\n",
    "\n",
    "#### 1. MDE (Minimum Detectable Effect)\n",
    "* MDE is a mix of data analysis, statistics, and business acumen. \n",
    "* Knowing total users, mean and variance of our success and guardrail metric and what our business needs we can defind the MDE. \n",
    "* There is often a tradeoff between precision and practicallity, So we may want to have the smallest possible MDE, but with the restriction of oru use base. \n",
    "* we want the **MDE** to be bigger than the **Standard deviation** of the metric.\n",
    "\n",
    "\n",
    "For our usecase; std of **DAU** is 90. so set 91 minimum difference between test and control group. This equals to approximately 0.33% increase. We need to find 0.33% difference between contorl and test group. \n",
    "\n",
    "Similarly, std of **CTR** is 1.7. So set 1.8% difference betwen test and contorl group. This equals to approximatley 5.5% increase. \n",
    "\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/our_mde.png\" width=\"500px\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is Statistical Significance?\n",
    "\n",
    "Statistical Significance allows us to answer the question of **how can we be sure that the test results will repeat?**. we define the level of Statistical significance (1-alpha) to set an acceptable level of mistakenly rejecting the Null Hypothesis - thinking we see an impact we expected when there is none. False positive. \n",
    "\n",
    "Most commonly alpha is between 1 to 10% and for digital expermients having alphaf of 5%. This means that if we run the test 100 times, in 5 times out of a hundred we will have a false positive result.  When alpha = 5%, we have statitical significance of 95%. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/statistical_significance.png\" width=\"500px\"></p>\n",
    "\n",
    "Different types of test requires different levels of signifiance, when testing anything like website you may be ok with 5% of false positives. Our ability to reach the proper level of statistical significance depends on the metric we target, the length of the experiment and the number of exposed users. \n",
    "\n",
    "Now to make sure that we we are not rejecting a successful test, we need to introduce the **power of the test**. \n",
    "\n",
    "#### 3. Power of the test (Beta)\n",
    "\n",
    "It's essentially tells that, **how can we know that we are not rejecting a successful test?**. We define the test power ( 1 - B(beta symbol)- beta) to set a level of mistakenly acceptin the null hypthesis -  thinking we don't have an impact when there is an impact. Falst negative. \n",
    "\n",
    "Usually beta is between 10% to 20% with mostly commonly in digital experiments 20%. This means that if we run the test 100 times, in 20 times out of a 100 we wil have a false negative result. When Beta is 20, we have the test power of 80%. (Thinking there is no impact when there is a impact)\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/power_beta.png\" width=\"500px\"></p>\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Type 1 and 2 Errors: \n",
    "- Type I Error (False Positive):\n",
    "    - A Type I error occurs when we incorrectly reject the null hypothesis (H₀) when it is actually true.\n",
    "    - In other words, we make a false positive conclusion.\n",
    "    - Imagine an innocent person being wrongly convicted in a court trial.\n",
    "    - The probability of making a Type I error is denoted by the significance level (α).\n",
    "    - It represents the risk of falsely claiming an effect exists when it doesn't.\n",
    "- Type II Error (False Negative):\n",
    "    - A Type II error happens when we fail to reject the null hypothesis (H₀) even though it is actually false.\n",
    "    - This means we miss a genuine effect.\n",
    "    - Consider a guilty person who escapes conviction due to this error.\n",
    "    - The probability of making a Type II error is denoted by beta (β).\n",
    "    - It reflects the risk of failing to detect an effect that truly exists.\n",
    "- Trade-off Between Type I and Type II Errors:\n",
    "    - Reducing one type of error often increases the risk of the other.\n",
    "    - Researchers must strike a balance based on the context and consequences.\n",
    "    - Adjusting the sample size, study design, and statistical power can help manage these risks.\n",
    "- Example: COVID-19 Testing:\n",
    "    - Suppose you get tested for COVID-19 due to mild symptoms.\n",
    "    - Type I error (false positive): The test result says you have coronavirus, but you actually don't.\n",
    "    - Type II error (false negative): The test result says you don't have coronavirus, but you actually do.\n",
    "\n",
    "Based on your usercase: \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/types.png\" width=\"500px\"></p>\n",
    "---\n",
    "<p style=\"text-align:center;\"><img src=\"images/tyeps2.png\" width=\"500px\"></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the sample size of an A/B Test \n",
    "\n",
    "For Sample size calculation we will use a **Z-test**. It's a statistical test to determine whether **two population means are different when the variance is known, and the sample size is large**. If the **sample size is small and the variance are not known** we will use **T-test** however this is very rare. \n",
    "\n",
    "A z-test is a hypothesis test in which the z-static follows a normal distribution. We will need the z-statistic to determine the sample size. We expect to have independent observations (CTR, DAU), on a randomly selected dataset with a sample size over 30. (For a smaller sample size we use T-test, but its' very rare in digital expermients)\n",
    "\n",
    "How to decide what test do we need to use? \n",
    "\n",
    "\n",
    "WE need to find the critical values for \"when to reject the null hypothesis**. Critical values can be found by **Z table**. Similary we can find the critical value for the the 1-Beta. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of metrics to calculate A/B testing performance\n",
    "\n",
    "Two types of metrics in calculating the A/B testing performacne \n",
    "1. Binomail Metrics \n",
    "2. Continuous Metrics \n",
    "\n",
    "\n",
    "#### 1. Binaomail Metric\n",
    "\n",
    "Basically the outcome takes on a value of either 0 or 1. The action is either present or not. Very often it defins a rate of something like \"conversaion rate\" or \"porportion of some sort\". In our case is CTR. \n",
    "\n",
    "To calculate the Bnomail metric we use **2 tailed z test**. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/binomail.png\" width=\"500px\"></p>\n",
    "\n",
    "#### 2. Continuous Metric\n",
    "\n",
    "This metric takes value on a scale. Usually those are the metrics per user like revenue per user, activity per user, number of view. It could be metrics per day, for example DAU. However when we calculate sample size for DAU, our sample becomes number of days rather than number of users needed. In some cases, it is beneficial to convert the continuous metrics into binomail to ease the process of setting up the A/B test. \n",
    "\n",
    "To calcaulte the sample size of continous metric we use **2 tailed z test**. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/continuous.png\" width=\"500px\"></p>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the A/B test\n",
    "\n",
    "Before starting the A/B testing, we need to make sure this things: \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/start.png\" width=\"500px\"></p>\n",
    "\n",
    "Usually we start with the **assignment procces** first. Usually a randomised test assignment is done based on the user_id, however, it's important to make sure there is no pretest bias between the groups. **pre-test** bias happens when users are not completely randomly shuffled between the groups and one of the groups ends up with a singficantly diffferent mean of any tracked metric compare to the other group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
