{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to calculate the **sample size** that we need for the test to have a **statistical significance** and also we'll see **how long do we need to run the test for?** based on the metrics. \n",
    "\n",
    "Again this is our usecase hypothesis: \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/our_hypothesis.png\" width=\"500px\"></p>\n",
    "\n",
    "\n",
    "After finding the sample size, we need to define \n",
    "1. When we will start the A/B test? \n",
    "2. How long will it run for? \n",
    "3. How many users will be exposesd? \n",
    "\n",
    "Thare are many reasons to finding those things, like your company already running a lot of **a/b test** or your new sample is already exposed to some other feature. So decide based on what you have. \n",
    "\n",
    "### Deciding the group sizes and duration\n",
    "\n",
    "This is the very crucial part because based on selecting the group size, you will roll out the new feature to all others. So the selected group size will replicate the whold group. So To decide the **test duration** and **group size**, we need to know: \n",
    "\n",
    "    * Minimum detectable effect (MDE)\n",
    "    * Power -  beta - type 2 error rate \n",
    "    * Significance level - p-value - type 1 error rate \n",
    "\n",
    "Before looking at this, please understand what is **test group** and **control group**\n",
    "\n",
    "**Test Group**: This group is exposed to the new or modified version of the thing being tested. This could be a new design for a website page, a different headline for an ad, or a variation in the wording of a call to action button.\n",
    "\n",
    "**Control Group**: This group serves as a baseline for comparison. They are exposed to the original version of the thing being tested. The control group helps to isolate the effect of the change being tested in the test group. By comparing the results of the two groups, you can see if the new version had a positive, negative, or neutral impact.\n",
    "\n",
    "\n",
    "\n",
    "#### 1. MDE (Minimum Detectable Effect)\n",
    "* MDE is a mix of data analysis, statistics, and business acumen. \n",
    "* Knowing total users, mean and variance of our success and guardrail metric and what our business needs we can defind the MDE. \n",
    "* There is often a tradeoff between precision and practicallity, So we may want to have the smallest possible MDE, but with the restriction of oru use base. \n",
    "* we want the **MDE** to be bigger than the **Standard deviation** of the metric.\n",
    "\n",
    "\n",
    "For our usecase; std of **DAU** is 90. so set 91 minimum difference between test and control group. This equals to approximately 0.33% increase. We need to find 0.33% difference between contorl and test group. \n",
    "\n",
    "Similarly, std of **CTR** is 1.7. So set 1.8% difference betwen test and contorl group. This equals to approximatley 5.5% increase. \n",
    "\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/our_mde.png\" width=\"500px\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is Statistical Significance?\n",
    "\n",
    "Statistical Significance allows us to answer the question of **how can we be sure that the test results will repeat?**. we define the level of Statistical significance (1-alpha) to set an acceptable level of mistakenly rejecting the Null Hypothesis - thinking we see an impact we expected when there is none. False positive. \n",
    "\n",
    "Most commonly alpha is between 1 to 10% and for digital expermients having alphaf of 5%. This means that if we run the test 100 times, in 5 times out of a hundred we will have a false positive result.  When alpha = 5%, we have statitical significance of 95%. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/statistical_significance.png\" width=\"500px\"></p>\n",
    "\n",
    "Different types of test requires different levels of signifiance, when testing anything like website you may be ok with 5% of false positives. Our ability to reach the proper level of statistical significance depends on the metric we target, the length of the experiment and the number of exposed users. \n",
    "\n",
    "Now to make sure that we we are not rejecting a successful test, we need to introduce the **power of the test**. \n",
    "\n",
    "#### 3. Power of the test (Beta)\n",
    "\n",
    "It's essentially tells that, **how can we know that we are not rejecting a successful test?**. We define the test power ( 1 - B(beta symbol)- beta) to set a level of mistakenly acceptin the null hypthesis -  thinking we don't have an impact when there is an impact. Falst negative. \n",
    "\n",
    "Usually beta is between 10% to 20% with mostly commonly in digital experiments 20%. This means that if we run the test 100 times, in 20 times out of a 100 we wil have a false negative result. When Beta is 20, we have the test power of 80%. (Thinking there is no impact when there is a impact)\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/power_beta.png\" width=\"500px\"></p>\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Type 1 and 2 Errors: \n",
    "- Type I Error (False Positive):\n",
    "    - A Type I error occurs when we incorrectly reject the null hypothesis (H₀) when it is actually true.\n",
    "    - In other words, we make a false positive conclusion.\n",
    "    - Imagine an innocent person being wrongly convicted in a court trial.\n",
    "    - The probability of making a Type I error is denoted by the significance level (α).\n",
    "    - It represents the risk of falsely claiming an effect exists when it doesn't.\n",
    "- Type II Error (False Negative):\n",
    "    - A Type II error happens when we fail to reject the null hypothesis (H₀) even though it is actually false.\n",
    "    - This means we miss a genuine effect.\n",
    "    - Consider a guilty person who escapes conviction due to this error.\n",
    "    - The probability of making a Type II error is denoted by beta (β).\n",
    "    - It reflects the risk of failing to detect an effect that truly exists.\n",
    "- Trade-off Between Type I and Type II Errors:\n",
    "    - Reducing one type of error often increases the risk of the other.\n",
    "    - Researchers must strike a balance based on the context and consequences.\n",
    "    - Adjusting the sample size, study design, and statistical power can help manage these risks.\n",
    "- Example: COVID-19 Testing:\n",
    "    - Suppose you get tested for COVID-19 due to mild symptoms.\n",
    "    - Type I error (false positive): The test result says you have coronavirus, but you actually don't.\n",
    "    - Type II error (false negative): The test result says you don't have coronavirus, but you actually do.\n",
    "\n",
    "Based on your usercase: \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/types.png\" width=\"500px\"></p>\n",
    "---\n",
    "<p style=\"text-align:center;\"><img src=\"images/tyeps2.png\" width=\"500px\"></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the sample size of an A/B Test \n",
    "\n",
    "For Sample size calculation we will use a **Z-test**. It's a statistical test to determine whether **two population means are different when the variance is known, and the sample size is large**. If the **sample size is small and the variance are not known** we will use **T-test** however this is very rare. \n",
    "\n",
    "A z-test is a hypothesis test in which the z-static follows a normal distribution. We will need the z-statistic to determine the sample size. We expect to have independent observations (CTR, DAU), on a randomly selected dataset with a sample size over 30. (For a smaller sample size we use T-test, but its' very rare in digital expermients)\n",
    "\n",
    "How to decide what test do we need to use? \n",
    "\n",
    "\n",
    "WE need to find the critical values for \"when to reject the null hypothesis**. Critical values can be found by **Z table**. Similary we can find the critical value for the the 1-Beta. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of metrics to calculate A/B testing performance\n",
    "\n",
    "Two types of metrics in calculating the A/B testing performacne \n",
    "1. Binomail Metrics \n",
    "2. Continuous Metrics \n",
    "\n",
    "\n",
    "#### 1. **Binaomail Metric**\n",
    "\n",
    "Basically the outcome takes on a value of either 0 or 1. The action is either present or not. Very often it defins a rate of something like **conversaion rate** or **porportion of some sort**. In our case is CTR. \n",
    "\n",
    "To calculate the Binamail metric we use **2 tailed z test**. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/binomail.png\" width=\"500px\"></p>\n",
    "\n",
    "Once you get all the value of this, you can find the sample size. The **sample size** is result of the bellow formula, You can see **8796.48**, this is the sample size: \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/sample_size_binomail.png\" width=\"500px\"></p>\n",
    "\n",
    "It means, we need to have **8796** people in each group, provided the group are **50%** split. \n",
    "\n",
    "\n",
    "#### 2. **Continuous Metric**\n",
    "\n",
    "This metric takes value on a scale. Usually those are the metrics per user like revenue per user, activity per user, number of view. It could be metrics per day, for example **DAU**. However when we calculate sample size for DAU, our sample becomes number of days rather than number of users needed. In some cases, it is beneficial to convert the continuous metrics into binomail to ease the process of setting up the A/B test. \n",
    "\n",
    "To calcaulte the sample size of continous metric we use **2 tailed z test**. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/continuous.png\" width=\"500px\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the A/B test\n",
    "\n",
    "Before starting the A/B testing, we need to make sure this things: \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/start.png\" width=\"500px\"></p>\n",
    "\n",
    "Usually we start with the **assignment procces** first. Usually a randomised test assignment is done based on the user_id, however, it's important to make sure there is no pretest bias between the groups. **pre-test** bias happens when users are not completely randomly shuffled between the groups and one of the groups ends up with a singficantly diffferent mean of any tracked metric compare to the other group. \n",
    "\n",
    "To avoidd pre-test bias companis run pre-assignment analysis, where thy \"assign\" users randomly based on mulitple **different seeds** and calcualte the difference in the metrics. Then select the seed which has **no** potential bias.\n",
    "\n",
    "Before we start analyzing the test based on the success criteria we need to do the test monitoring, it's also called **performance monitoring**, this is to make sure there is no **negative impact** on the baseline of the company. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/perf.png\" width=\"500px\"></p>\n",
    "\n",
    "Now we need to understand the term **peeking**. Peeking is a **issue** of multiple test result calculations. the more you check the significance of your succes metrics, the more likely you are to see a false positive. When we monitor the test, we only look and make decisions based on the critical business metrics ( number of erros, crashes ), not the **success metrics**. It's also called **P-hacking**, **inflation-bias**, or **selective reporting** or **significance chasing**. And this is the practice of calculating the P-value uup to the point where it reaches statistical significance in the preferred directioon and basing the success of the test on that observation of course. \n",
    "\n",
    "You should not make any decisions from peeking before the actual test duration that you've calcualted for your reaching of the significance.Then after that you can start the analysis process. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/analysis.png\" width=\"500px\"></p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to find which hypothesis testing to use? \n",
    "\n",
    "1. ONe tailed test \n",
    "\n",
    "* If you only want to know that test group is bigger than the control group, we will use alpha to cut off and calclulate the test statistics. If our **mean** for the test group ends on the **left side** of the point we **don't** reject the **null hypothesis**. If **mean** of the test group ends up on **right side** we will reject **null hypothesis**. \n",
    "\n",
    "\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/simple_hype.png\" width=\"500px\"></p>\n",
    "\n",
    "\n",
    "2. Two tailed test\n",
    "* If you want to know the **test metrics** is smaller or bigger than the **control group**, we will use the two tailed test. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/two_tailed_test.png\" width=\"500px\"></p>\n",
    "\n",
    "This (bellow) pitcure illustrates where we will **Not reject** the **null hypothesis**\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/two_tailed_not_reject.png\" width=\"500px\"></p> \n",
    "\n",
    "Because our **test group mean** is lesser than the alpha. So we're not rejecting the **null hypothesis**\n",
    "\n",
    "Bellow pitcture illustrates where will **reject** the **null hypothesis**\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/two_tailed_reject.png\" width=\"500px\"></p>\n",
    "\n",
    "Then we will say, there is a **difference** between **test group** and **control group**> \n",
    "\n",
    "So we need to find the **critical value** to when to reject the **Null hypothesis**. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/critical_value.png\" width=\"500px\"></p>\n",
    "\n",
    "\n",
    "This **critical value** can be found by using the **z table**. We can find the Z value based on the desired statistical significance ( pvalue ). \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
