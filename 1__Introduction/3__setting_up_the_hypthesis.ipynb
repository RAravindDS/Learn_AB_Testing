{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to calculate the **sample size** that we need for the test to have a **statistical significance** and also we'll see **how long do we need to run the test for?** based on the metrics. \n",
    "\n",
    "Again this is our usecase hypothesis: \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/our_hypothesis.png\" width=\"500px\"></p>\n",
    "\n",
    "\n",
    "After finding the sample size, we need to define \n",
    "1. When we will start the A/B test? \n",
    "2. How long will it run for? \n",
    "3. How many users will be exposesd? \n",
    "\n",
    "Thare are many reasons to finding those things, like your company already running a lot of **a/b test** or your new sample is already exposed to some other feature. So decide based on what you have. \n",
    "\n",
    "### Deciding the group sizes and duration\n",
    "\n",
    "This is the very crucial part because based on selecting the group size, you will roll out the new feature to all others. So the selected group size will replicate the whold group. So To decide the **test duration** and **group size**, we need to know: \n",
    "\n",
    "    * Minimum detectable effect (MDE)\n",
    "    * Power -  beta - type 2 error rate \n",
    "    * Significance level - p-value - type 1 error rate \n",
    "\n",
    "Before looking at this, please understand what is **test group** and **control group**\n",
    "\n",
    "**Test Group**: This group is exposed to the new or modified version of the thing being tested. This could be a new design for a website page, a different headline for an ad, or a variation in the wording of a call to action button.\n",
    "\n",
    "**Control Group**: This group serves as a baseline for comparison. They are exposed to the original version of the thing being tested. The control group helps to isolate the effect of the change being tested in the test group. By comparing the results of the two groups, you can see if the new version had a positive, negative, or neutral impact.\n",
    "\n",
    "\n",
    "\n",
    "#### 1. MDE (Minimum Detectable Effect)\n",
    "* MDE is a mix of data analysis, statistics, and business acumen. \n",
    "* Knowing total users, mean and variance of our success and guardrail metric and what our business needs we can defind the MDE. \n",
    "* There is often a tradeoff between precision and practicallity, So we may want to have the smallest possible MDE, but with the restriction of oru use base. \n",
    "* we want the **MDE** to be bigger than the **Standard deviation** of the metric.\n",
    "\n",
    "\n",
    "For our usecase; std of **DAU** is 90. so set 91 minimum difference between test and control group. This equals to approximately 0.33% increase. We need to find 0.33% difference between contorl and test group. \n",
    "\n",
    "Similarly, std of **CTR** is 1.7. So set 1.8% difference betwen test and contorl group. This equals to approximatley 5.5% increase. \n",
    "\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/our_mde.png\" width=\"500px\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is Statistical Significance?\n",
    "\n",
    "Statistical Significance allows us to answer the question of **how can we be sure that the test results will repeat?**. we define the level of Statistical significance (1-alpha) to set an acceptable level of mistakenly rejecting the Null Hypothesis - thinking we see an impact we expected when there is none. False positive. \n",
    "\n",
    "Most commonly alpha is between 1 to 10% and for digital expermients having alphaf of 5%. This means that if we run the test 100 times, in 5 times out of a hundred we will have a false positive result.  When alpha = 5%, we have statitical significance of 95%. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/statistical_significance.png\" width=\"500px\"></p>\n",
    "\n",
    "Different types of test requires different levels of signifiance, when testing anything like website you may be ok with 5% of false positives. Our ability to reach the proper level of statistical significance depends on the metric we target, the length of the experiment and the number of exposed users. \n",
    "\n",
    "Now to make sure that we we are not rejecting a successful test, we need to introduce the **power of the test**. \n",
    "\n",
    "#### 3. Power of the test (Beta)\n",
    "\n",
    "It's essentially tells that, **how can we know that we are not rejecting a successful test?**. We define the test power ( 1 - B(beta symbol)- beta) to set a level of mistakenly acceptin the null hypthesis -  thinking we don't have an impact when there is an impact. Falst negative. \n",
    "\n",
    "Usually beta is between 10% to 20% with mostly commonly in digital experiments 20%. This means that if we run the test 100 times, in 20 times out of a 100 we wil have a false negative result. When Beta is 20, we have the test power of 80%. (Thinking there is no impact when there is a impact)\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/power_beta.png\" width=\"500px\"></p>\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Type 1 and 2 Errors: \n",
    "- Type I Error (False Positive):\n",
    "    - A Type I error occurs when we incorrectly reject the null hypothesis (H₀) when it is actually true.\n",
    "    - In other words, we make a false positive conclusion.\n",
    "    - Imagine an innocent person being wrongly convicted in a court trial.\n",
    "    - The probability of making a Type I error is denoted by the significance level (α).\n",
    "    - It represents the risk of falsely claiming an effect exists when it doesn't.\n",
    "- Type II Error (False Negative):\n",
    "    - A Type II error happens when we fail to reject the null hypothesis (H₀) even though it is actually false.\n",
    "    - This means we miss a genuine effect.\n",
    "    - Consider a guilty person who escapes conviction due to this error.\n",
    "    - The probability of making a Type II error is denoted by beta (β).\n",
    "    - It reflects the risk of failing to detect an effect that truly exists.\n",
    "- Trade-off Between Type I and Type II Errors:\n",
    "    - Reducing one type of error often increases the risk of the other.\n",
    "    - Researchers must strike a balance based on the context and consequences.\n",
    "    - Adjusting the sample size, study design, and statistical power can help manage these risks.\n",
    "- Example: COVID-19 Testing:\n",
    "    - Suppose you get tested for COVID-19 due to mild symptoms.\n",
    "    - Type I error (false positive): The test result says you have coronavirus, but you actually don't.\n",
    "    - Type II error (false negative): The test result says you don't have coronavirus, but you actually do.\n",
    "\n",
    "Based on your usercase: \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/types.png\" width=\"500px\"></p>\n",
    "---\n",
    "<p style=\"text-align:center;\"><img src=\"images/tyeps2.png\" width=\"500px\"></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
